{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.logging import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set gpu id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> download the pretrained weights of roberta model\n",
    "\n",
    "**Run only once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"roberta-base\")\n",
    "\n",
    "tokenizer.save_pretrained('roberta-base')\n",
    "model.save_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=96):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'selected_text' in df\n",
    "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "            vocab_file='roberta-base/vocab.json', \n",
    "            merges_file='roberta-base/merges.txt', \n",
    "            lowercase=True,\n",
    "            add_prefix_space=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        ids, masks, tweet, offsets = self.get_input_data(row)\n",
    "        data['ids'] = ids\n",
    "        data['masks'] = masks\n",
    "        data['tweet'] = tweet\n",
    "        data['offsets'] = offsets\n",
    "        \n",
    "        if self.labeled:\n",
    "            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
    "            data['start_idx'] = start_idx\n",
    "            data['end_idx'] = end_idx\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_input_data(self, row):\n",
    "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "        encoding = self.tokenizer.encode(tweet)\n",
    "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
    "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
    "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "                \n",
    "        pad_len = self.max_len - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids += [1] * pad_len\n",
    "            offsets += [(0, 0)] * pad_len\n",
    "        \n",
    "        ids = torch.tensor(ids)\n",
    "        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "        offsets = torch.tensor(offsets)\n",
    "        \n",
    "        return ids, masks, tweet, offsets\n",
    "        \n",
    "    def get_target_idx(self, row, tweet, offsets):\n",
    "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "        start_idx = target_idx[0]\n",
    "        end_idx = target_idx[-1]\n",
    "        \n",
    "        return start_idx, end_idx\n",
    "        \n",
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df  = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].astype(str)\n",
    "train_df['selected_text'] = train_df['selected_text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda x: ' '.join(x.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df.selected_text = train_df.selected_text.apply(lambda x: ' '.join(x.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = train_df[(train_df.sentiment=='neutral') & (train_df.text != train_df.selected_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "has_link =  lambda text : 'http' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x.text.map(has_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x.to_csv('data/neutral_not_eq.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)    \n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TweetModel(pl.LightningModule):\n",
    "    def __init__(self, lr):\n",
    "        super(TweetModel, self).__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            'roberta-base/config.json', output_hidden_states=True)    \n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            'roberta-base/pytorch_model.bin', config=config)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(config.hidden_size, 32)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        self.criterion = loss_fn\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "         \n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(self.dropout2(self.fc1(x)))\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids = batch['ids'].cuda()\n",
    "        masks = batch['masks'].cuda()\n",
    "        tweet = batch['tweet']\n",
    "        offsets = batch['offsets'].numpy()\n",
    "        start_idx = batch['start_idx'].cuda()\n",
    "        end_idx = batch['end_idx'].cuda()\n",
    "        \n",
    "        start_logits, end_logits = self.forward(ids, masks)\n",
    "        loss = self.criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "        \n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW([p for p in self.parameters() if p.requires_grad],\n",
    "                                 lr=self.lr, betas=(0.9, 0.999))\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids = batch['ids'].cuda()\n",
    "        masks = batch['masks'].cuda()\n",
    "        tweet = batch['tweet']\n",
    "        offsets = batch['offsets'].numpy()\n",
    "        start_idx = batch['start_idx'].cuda()\n",
    "        end_idx = batch['end_idx'].cuda()\n",
    "        start_logits, end_logits = self.forward(ids, masks)\n",
    "        loss = self.criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "        \n",
    "        start_idx = start_idx.cpu().detach().numpy()\n",
    "        end_idx = end_idx.cpu().detach().numpy()\n",
    "        start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "        end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "        \n",
    "        batch_jaccard = 0\n",
    "        for i in range(len(ids)):                        \n",
    "            jaccard_score = compute_jaccard_score(\n",
    "                tweet[i],\n",
    "                start_idx[i],\n",
    "                end_idx[i],\n",
    "                start_logits[i], \n",
    "                end_logits[i], \n",
    "                offsets[i])\n",
    "            batch_jaccard += jaccard_score\n",
    "        \n",
    "        batch_len = len(ids)\n",
    "        \n",
    "        return {'val_loss':loss, 'val_jaccard':batch_jaccard,\n",
    "               'batch_len':batch_len}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        total_len = sum([batch['batch_len'] for batch in outputs])\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        total_jaccard = sum([x['val_jaccard'] for x in outputs])\n",
    "        \n",
    "        total_jaccard = torch.tensor(total_jaccard / total_len)\n",
    "        total_jaccard = torch.tensor([total_jaccard])\n",
    "        \n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_jaccard': total_jaccard}\n",
    "        \n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs,\n",
    "               'progress_bar': tensorboard_logs}\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return dataloaders_dict['train']\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return dataloaders_dict['val']\n",
    "    \n",
    "    def test_step(self, batch, batch_id):\n",
    "        ids     = batch['ids'].cuda()\n",
    "        masks   = batch['masks'].cuda()\n",
    "        tweet   = batch['tweet'].cuda()\n",
    "        offsets = batch['offsets'].numpy()\n",
    "        \n",
    "        output = self.forward(ids, masks)\n",
    "        start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "        end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        \n",
    "        return {'start_pred':start_pred, 'end_pred': end_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_selected_text(text, start_idx, end_idx, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"  # add spaces for words in b/w\n",
    "    return selected_text\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
    "    ''' out model outputs 2 vectors\n",
    "    start logits -> probability that a token is the start\n",
    "    end logits   -> probability that a token is the ending'''\n",
    "    start_pred = np.argmax(start_logits)  # start pred\n",
    "    end_pred = np.argmax(end_logits)      # end pred\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "        \n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "    \n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_total_params(model):\n",
    "    s = 0\n",
    "    for param in model.parameters():\n",
    "        s += param.numel()\n",
    "    return s\n",
    "\n",
    "def get_trainable_params(model):\n",
    "    s = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            s += param.numel()\n",
    "    return s\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name)\n",
    "\n",
    "def finetune_model(model, last_n):\n",
    "    '''freezes all the layers in the model except last_n number of layers\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    model  :  pytorch model\n",
    "    last_n    :  number of last layers to unfreeze\n",
    "    '''\n",
    "    \n",
    "    total_layers = len(list(model.parameters()))\n",
    "    \n",
    "    for enum, param in enumerate(model.parameters()):\n",
    "        param.requires_grad = False\n",
    "        if enum + last_n >= total_layers:\n",
    "            param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": "false",
    "jupyter": {
     "outputs_hidden": "false"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "\n",
      "    | Name                                                | Type              | Params\n",
      "--------------------------------------------------------------------------------------\n",
      "0   | roberta                                             | RobertaModel      | 124 M \n",
      "1   | roberta.embeddings                                  | RobertaEmbeddings | 39 M  \n",
      "2   | roberta.embeddings.word_embeddings                  | Embedding         | 38 M  \n",
      "3   | roberta.embeddings.position_embeddings              | Embedding         | 394 K \n",
      "4   | roberta.embeddings.token_type_embeddings            | Embedding         | 768   \n",
      "5   | roberta.embeddings.LayerNorm                        | LayerNorm         | 1 K   \n",
      "6   | roberta.embeddings.dropout                          | Dropout           | 0     \n",
      "7   | roberta.encoder                                     | BertEncoder       | 85 M  \n",
      "8   | roberta.encoder.layer                               | ModuleList        | 85 M  \n",
      "9   | roberta.encoder.layer.0                             | BertLayer         | 7 M   \n",
      "10  | roberta.encoder.layer.0.attention                   | BertAttention     | 2 M   \n",
      "11  | roberta.encoder.layer.0.attention.self              | BertSelfAttention | 1 M   \n",
      "12  | roberta.encoder.layer.0.attention.self.query        | Linear            | 590 K \n",
      "13  | roberta.encoder.layer.0.attention.self.key          | Linear            | 590 K \n",
      "14  | roberta.encoder.layer.0.attention.self.value        | Linear            | 590 K \n",
      "15  | roberta.encoder.layer.0.attention.self.dropout      | Dropout           | 0     \n",
      "16  | roberta.encoder.layer.0.attention.output            | BertSelfOutput    | 592 K \n",
      "17  | roberta.encoder.layer.0.attention.output.dense      | Linear            | 590 K \n",
      "18  | roberta.encoder.layer.0.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "19  | roberta.encoder.layer.0.attention.output.dropout    | Dropout           | 0     \n",
      "20  | roberta.encoder.layer.0.intermediate                | BertIntermediate  | 2 M   \n",
      "21  | roberta.encoder.layer.0.intermediate.dense          | Linear            | 2 M   \n",
      "22  | roberta.encoder.layer.0.output                      | BertOutput        | 2 M   \n",
      "23  | roberta.encoder.layer.0.output.dense                | Linear            | 2 M   \n",
      "24  | roberta.encoder.layer.0.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "25  | roberta.encoder.layer.0.output.dropout              | Dropout           | 0     \n",
      "26  | roberta.encoder.layer.1                             | BertLayer         | 7 M   \n",
      "27  | roberta.encoder.layer.1.attention                   | BertAttention     | 2 M   \n",
      "28  | roberta.encoder.layer.1.attention.self              | BertSelfAttention | 1 M   \n",
      "29  | roberta.encoder.layer.1.attention.self.query        | Linear            | 590 K \n",
      "30  | roberta.encoder.layer.1.attention.self.key          | Linear            | 590 K \n",
      "31  | roberta.encoder.layer.1.attention.self.value        | Linear            | 590 K \n",
      "32  | roberta.encoder.layer.1.attention.self.dropout      | Dropout           | 0     \n",
      "33  | roberta.encoder.layer.1.attention.output            | BertSelfOutput    | 592 K \n",
      "34  | roberta.encoder.layer.1.attention.output.dense      | Linear            | 590 K \n",
      "35  | roberta.encoder.layer.1.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "36  | roberta.encoder.layer.1.attention.output.dropout    | Dropout           | 0     \n",
      "37  | roberta.encoder.layer.1.intermediate                | BertIntermediate  | 2 M   \n",
      "38  | roberta.encoder.layer.1.intermediate.dense          | Linear            | 2 M   \n",
      "39  | roberta.encoder.layer.1.output                      | BertOutput        | 2 M   \n",
      "40  | roberta.encoder.layer.1.output.dense                | Linear            | 2 M   \n",
      "41  | roberta.encoder.layer.1.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "42  | roberta.encoder.layer.1.output.dropout              | Dropout           | 0     \n",
      "43  | roberta.encoder.layer.2                             | BertLayer         | 7 M   \n",
      "44  | roberta.encoder.layer.2.attention                   | BertAttention     | 2 M   \n",
      "45  | roberta.encoder.layer.2.attention.self              | BertSelfAttention | 1 M   \n",
      "46  | roberta.encoder.layer.2.attention.self.query        | Linear            | 590 K \n",
      "47  | roberta.encoder.layer.2.attention.self.key          | Linear            | 590 K \n",
      "48  | roberta.encoder.layer.2.attention.self.value        | Linear            | 590 K \n",
      "49  | roberta.encoder.layer.2.attention.self.dropout      | Dropout           | 0     \n",
      "50  | roberta.encoder.layer.2.attention.output            | BertSelfOutput    | 592 K \n",
      "51  | roberta.encoder.layer.2.attention.output.dense      | Linear            | 590 K \n",
      "52  | roberta.encoder.layer.2.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "53  | roberta.encoder.layer.2.attention.output.dropout    | Dropout           | 0     \n",
      "54  | roberta.encoder.layer.2.intermediate                | BertIntermediate  | 2 M   \n",
      "55  | roberta.encoder.layer.2.intermediate.dense          | Linear            | 2 M   \n",
      "56  | roberta.encoder.layer.2.output                      | BertOutput        | 2 M   \n",
      "57  | roberta.encoder.layer.2.output.dense                | Linear            | 2 M   \n",
      "58  | roberta.encoder.layer.2.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "59  | roberta.encoder.layer.2.output.dropout              | Dropout           | 0     \n",
      "60  | roberta.encoder.layer.3                             | BertLayer         | 7 M   \n",
      "61  | roberta.encoder.layer.3.attention                   | BertAttention     | 2 M   \n",
      "62  | roberta.encoder.layer.3.attention.self              | BertSelfAttention | 1 M   \n",
      "63  | roberta.encoder.layer.3.attention.self.query        | Linear            | 590 K \n",
      "64  | roberta.encoder.layer.3.attention.self.key          | Linear            | 590 K \n",
      "65  | roberta.encoder.layer.3.attention.self.value        | Linear            | 590 K \n",
      "66  | roberta.encoder.layer.3.attention.self.dropout      | Dropout           | 0     \n",
      "67  | roberta.encoder.layer.3.attention.output            | BertSelfOutput    | 592 K \n",
      "68  | roberta.encoder.layer.3.attention.output.dense      | Linear            | 590 K \n",
      "69  | roberta.encoder.layer.3.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "70  | roberta.encoder.layer.3.attention.output.dropout    | Dropout           | 0     \n",
      "71  | roberta.encoder.layer.3.intermediate                | BertIntermediate  | 2 M   \n",
      "72  | roberta.encoder.layer.3.intermediate.dense          | Linear            | 2 M   \n",
      "73  | roberta.encoder.layer.3.output                      | BertOutput        | 2 M   \n",
      "74  | roberta.encoder.layer.3.output.dense                | Linear            | 2 M   \n",
      "75  | roberta.encoder.layer.3.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "76  | roberta.encoder.layer.3.output.dropout              | Dropout           | 0     \n",
      "77  | roberta.encoder.layer.4                             | BertLayer         | 7 M   \n",
      "78  | roberta.encoder.layer.4.attention                   | BertAttention     | 2 M   \n",
      "79  | roberta.encoder.layer.4.attention.self              | BertSelfAttention | 1 M   \n",
      "80  | roberta.encoder.layer.4.attention.self.query        | Linear            | 590 K \n",
      "81  | roberta.encoder.layer.4.attention.self.key          | Linear            | 590 K \n",
      "82  | roberta.encoder.layer.4.attention.self.value        | Linear            | 590 K \n",
      "83  | roberta.encoder.layer.4.attention.self.dropout      | Dropout           | 0     \n",
      "84  | roberta.encoder.layer.4.attention.output            | BertSelfOutput    | 592 K \n",
      "85  | roberta.encoder.layer.4.attention.output.dense      | Linear            | 590 K \n",
      "86  | roberta.encoder.layer.4.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "87  | roberta.encoder.layer.4.attention.output.dropout    | Dropout           | 0     \n",
      "88  | roberta.encoder.layer.4.intermediate                | BertIntermediate  | 2 M   \n",
      "89  | roberta.encoder.layer.4.intermediate.dense          | Linear            | 2 M   \n",
      "90  | roberta.encoder.layer.4.output                      | BertOutput        | 2 M   \n",
      "91  | roberta.encoder.layer.4.output.dense                | Linear            | 2 M   \n",
      "92  | roberta.encoder.layer.4.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "93  | roberta.encoder.layer.4.output.dropout              | Dropout           | 0     \n",
      "94  | roberta.encoder.layer.5                             | BertLayer         | 7 M   \n",
      "95  | roberta.encoder.layer.5.attention                   | BertAttention     | 2 M   \n",
      "96  | roberta.encoder.layer.5.attention.self              | BertSelfAttention | 1 M   \n",
      "97  | roberta.encoder.layer.5.attention.self.query        | Linear            | 590 K \n",
      "98  | roberta.encoder.layer.5.attention.self.key          | Linear            | 590 K \n",
      "99  | roberta.encoder.layer.5.attention.self.value        | Linear            | 590 K \n",
      "100 | roberta.encoder.layer.5.attention.self.dropout      | Dropout           | 0     \n",
      "101 | roberta.encoder.layer.5.attention.output            | BertSelfOutput    | 592 K \n",
      "102 | roberta.encoder.layer.5.attention.output.dense      | Linear            | 590 K \n",
      "103 | roberta.encoder.layer.5.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "104 | roberta.encoder.layer.5.attention.output.dropout    | Dropout           | 0     \n",
      "105 | roberta.encoder.layer.5.intermediate                | BertIntermediate  | 2 M   \n",
      "106 | roberta.encoder.layer.5.intermediate.dense          | Linear            | 2 M   \n",
      "107 | roberta.encoder.layer.5.output                      | BertOutput        | 2 M   \n",
      "108 | roberta.encoder.layer.5.output.dense                | Linear            | 2 M   \n",
      "109 | roberta.encoder.layer.5.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "110 | roberta.encoder.layer.5.output.dropout              | Dropout           | 0     \n",
      "111 | roberta.encoder.layer.6                             | BertLayer         | 7 M   \n",
      "112 | roberta.encoder.layer.6.attention                   | BertAttention     | 2 M   \n",
      "113 | roberta.encoder.layer.6.attention.self              | BertSelfAttention | 1 M   \n",
      "114 | roberta.encoder.layer.6.attention.self.query        | Linear            | 590 K \n",
      "115 | roberta.encoder.layer.6.attention.self.key          | Linear            | 590 K \n",
      "116 | roberta.encoder.layer.6.attention.self.value        | Linear            | 590 K \n",
      "117 | roberta.encoder.layer.6.attention.self.dropout      | Dropout           | 0     \n",
      "118 | roberta.encoder.layer.6.attention.output            | BertSelfOutput    | 592 K \n",
      "119 | roberta.encoder.layer.6.attention.output.dense      | Linear            | 590 K \n",
      "120 | roberta.encoder.layer.6.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "121 | roberta.encoder.layer.6.attention.output.dropout    | Dropout           | 0     \n",
      "122 | roberta.encoder.layer.6.intermediate                | BertIntermediate  | 2 M   \n",
      "123 | roberta.encoder.layer.6.intermediate.dense          | Linear            | 2 M   \n",
      "124 | roberta.encoder.layer.6.output                      | BertOutput        | 2 M   \n",
      "125 | roberta.encoder.layer.6.output.dense                | Linear            | 2 M   \n",
      "126 | roberta.encoder.layer.6.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "127 | roberta.encoder.layer.6.output.dropout              | Dropout           | 0     \n",
      "128 | roberta.encoder.layer.7                             | BertLayer         | 7 M   \n",
      "129 | roberta.encoder.layer.7.attention                   | BertAttention     | 2 M   \n",
      "130 | roberta.encoder.layer.7.attention.self              | BertSelfAttention | 1 M   \n",
      "131 | roberta.encoder.layer.7.attention.self.query        | Linear            | 590 K \n",
      "132 | roberta.encoder.layer.7.attention.self.key          | Linear            | 590 K \n",
      "133 | roberta.encoder.layer.7.attention.self.value        | Linear            | 590 K \n",
      "134 | roberta.encoder.layer.7.attention.self.dropout      | Dropout           | 0     \n",
      "135 | roberta.encoder.layer.7.attention.output            | BertSelfOutput    | 592 K \n",
      "136 | roberta.encoder.layer.7.attention.output.dense      | Linear            | 590 K \n",
      "137 | roberta.encoder.layer.7.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "138 | roberta.encoder.layer.7.attention.output.dropout    | Dropout           | 0     \n",
      "139 | roberta.encoder.layer.7.intermediate                | BertIntermediate  | 2 M   \n",
      "140 | roberta.encoder.layer.7.intermediate.dense          | Linear            | 2 M   \n",
      "141 | roberta.encoder.layer.7.output                      | BertOutput        | 2 M   \n",
      "142 | roberta.encoder.layer.7.output.dense                | Linear            | 2 M   \n",
      "143 | roberta.encoder.layer.7.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "144 | roberta.encoder.layer.7.output.dropout              | Dropout           | 0     \n",
      "145 | roberta.encoder.layer.8                             | BertLayer         | 7 M   \n",
      "146 | roberta.encoder.layer.8.attention                   | BertAttention     | 2 M   \n",
      "147 | roberta.encoder.layer.8.attention.self              | BertSelfAttention | 1 M   \n",
      "148 | roberta.encoder.layer.8.attention.self.query        | Linear            | 590 K \n",
      "149 | roberta.encoder.layer.8.attention.self.key          | Linear            | 590 K \n",
      "150 | roberta.encoder.layer.8.attention.self.value        | Linear            | 590 K \n",
      "151 | roberta.encoder.layer.8.attention.self.dropout      | Dropout           | 0     \n",
      "152 | roberta.encoder.layer.8.attention.output            | BertSelfOutput    | 592 K \n",
      "153 | roberta.encoder.layer.8.attention.output.dense      | Linear            | 590 K \n",
      "154 | roberta.encoder.layer.8.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "155 | roberta.encoder.layer.8.attention.output.dropout    | Dropout           | 0     \n",
      "156 | roberta.encoder.layer.8.intermediate                | BertIntermediate  | 2 M   \n",
      "157 | roberta.encoder.layer.8.intermediate.dense          | Linear            | 2 M   \n",
      "158 | roberta.encoder.layer.8.output                      | BertOutput        | 2 M   \n",
      "159 | roberta.encoder.layer.8.output.dense                | Linear            | 2 M   \n",
      "160 | roberta.encoder.layer.8.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "161 | roberta.encoder.layer.8.output.dropout              | Dropout           | 0     \n",
      "162 | roberta.encoder.layer.9                             | BertLayer         | 7 M   \n",
      "163 | roberta.encoder.layer.9.attention                   | BertAttention     | 2 M   \n",
      "164 | roberta.encoder.layer.9.attention.self              | BertSelfAttention | 1 M   \n",
      "165 | roberta.encoder.layer.9.attention.self.query        | Linear            | 590 K \n",
      "166 | roberta.encoder.layer.9.attention.self.key          | Linear            | 590 K \n",
      "167 | roberta.encoder.layer.9.attention.self.value        | Linear            | 590 K \n",
      "168 | roberta.encoder.layer.9.attention.self.dropout      | Dropout           | 0     \n",
      "169 | roberta.encoder.layer.9.attention.output            | BertSelfOutput    | 592 K \n",
      "170 | roberta.encoder.layer.9.attention.output.dense      | Linear            | 590 K \n",
      "171 | roberta.encoder.layer.9.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "172 | roberta.encoder.layer.9.attention.output.dropout    | Dropout           | 0     \n",
      "173 | roberta.encoder.layer.9.intermediate                | BertIntermediate  | 2 M   \n",
      "174 | roberta.encoder.layer.9.intermediate.dense          | Linear            | 2 M   \n",
      "175 | roberta.encoder.layer.9.output                      | BertOutput        | 2 M   \n",
      "176 | roberta.encoder.layer.9.output.dense                | Linear            | 2 M   \n",
      "177 | roberta.encoder.layer.9.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "178 | roberta.encoder.layer.9.output.dropout              | Dropout           | 0     \n",
      "179 | roberta.encoder.layer.10                            | BertLayer         | 7 M   \n",
      "180 | roberta.encoder.layer.10.attention                  | BertAttention     | 2 M   \n",
      "181 | roberta.encoder.layer.10.attention.self             | BertSelfAttention | 1 M   \n",
      "182 | roberta.encoder.layer.10.attention.self.query       | Linear            | 590 K \n",
      "183 | roberta.encoder.layer.10.attention.self.key         | Linear            | 590 K \n",
      "184 | roberta.encoder.layer.10.attention.self.value       | Linear            | 590 K \n",
      "185 | roberta.encoder.layer.10.attention.self.dropout     | Dropout           | 0     \n",
      "186 | roberta.encoder.layer.10.attention.output           | BertSelfOutput    | 592 K \n",
      "187 | roberta.encoder.layer.10.attention.output.dense     | Linear            | 590 K \n",
      "188 | roberta.encoder.layer.10.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "189 | roberta.encoder.layer.10.attention.output.dropout   | Dropout           | 0     \n",
      "190 | roberta.encoder.layer.10.intermediate               | BertIntermediate  | 2 M   \n",
      "191 | roberta.encoder.layer.10.intermediate.dense         | Linear            | 2 M   \n",
      "192 | roberta.encoder.layer.10.output                     | BertOutput        | 2 M   \n",
      "193 | roberta.encoder.layer.10.output.dense               | Linear            | 2 M   \n",
      "194 | roberta.encoder.layer.10.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "195 | roberta.encoder.layer.10.output.dropout             | Dropout           | 0     \n",
      "196 | roberta.encoder.layer.11                            | BertLayer         | 7 M   \n",
      "197 | roberta.encoder.layer.11.attention                  | BertAttention     | 2 M   \n",
      "198 | roberta.encoder.layer.11.attention.self             | BertSelfAttention | 1 M   \n",
      "199 | roberta.encoder.layer.11.attention.self.query       | Linear            | 590 K \n",
      "200 | roberta.encoder.layer.11.attention.self.key         | Linear            | 590 K \n",
      "201 | roberta.encoder.layer.11.attention.self.value       | Linear            | 590 K \n",
      "202 | roberta.encoder.layer.11.attention.self.dropout     | Dropout           | 0     \n",
      "203 | roberta.encoder.layer.11.attention.output           | BertSelfOutput    | 592 K \n",
      "204 | roberta.encoder.layer.11.attention.output.dense     | Linear            | 590 K \n",
      "205 | roberta.encoder.layer.11.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "206 | roberta.encoder.layer.11.attention.output.dropout   | Dropout           | 0     \n",
      "207 | roberta.encoder.layer.11.intermediate               | BertIntermediate  | 2 M   \n",
      "208 | roberta.encoder.layer.11.intermediate.dense         | Linear            | 2 M   \n",
      "209 | roberta.encoder.layer.11.output                     | BertOutput        | 2 M   \n",
      "210 | roberta.encoder.layer.11.output.dense               | Linear            | 2 M   \n",
      "211 | roberta.encoder.layer.11.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "212 | roberta.encoder.layer.11.output.dropout             | Dropout           | 0     \n",
      "213 | roberta.pooler                                      | BertPooler        | 590 K \n",
      "214 | roberta.pooler.dense                                | Linear            | 590 K \n",
      "215 | roberta.pooler.activation                           | Tanh              | 0     \n",
      "216 | dropout1                                            | Dropout           | 0     \n",
      "217 | fc1                                                 | Linear            | 24 K  \n",
      "218 | dropout2                                            | Dropout           | 0     \n",
      "219 | fc2                                                 | Linear            | 66    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390f9f0970e944fb96affee1cf9c1925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c90d52598744b6694ea10758705fadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0792c7dbaa438cb09541c831793171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680e9df9dfb34216b436586964213f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013adc89c2a74597903fcd801ff843eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tm = TweetModel(3e-5)\n",
    "\n",
    "# get_total_params(tm)\n",
    "# finetune_model(tm,4)\n",
    "# print_trainable_params(tm)\n",
    "# get_trainable_params(tm)\n",
    "\n",
    "name = 'roberta-2-layers-total-train'\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "                save_dir='ts-logs',\n",
    "                name = name\n",
    "            )\n",
    "\n",
    "early_stopping = EarlyStopping('val_loss',patience=5)\n",
    "\n",
    "if not os.path.exists(f'saved_models/{name}'):\n",
    "    os.mkdir(f'saved_models/{name}')\n",
    "checkpoint_callback = ModelCheckpoint(filepath=f'saved_models/{name}')\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(min_epochs=10,\n",
    "                     logger=logger,\n",
    "                     early_stop_callback=early_stopping,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "tm.cuda()\n",
    "\n",
    "trainer.fit(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": "false",
    "jupyter": {
     "outputs_hidden": "false"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n",
      "Epoch 1/6 | train | Loss: 2.2849 | Jaccard: 0.6532\n",
      "Epoch 1/6 |  val  | Loss: 1.5838 | Jaccard: 0.7264\n",
      "Epoch 2/6 | train | Loss: 1.6371 | Jaccard: 0.7124\n",
      "Epoch 2/6 |  val  | Loss: 1.5323 | Jaccard: 0.7196\n",
      "Epoch 3/6 | train | Loss: 1.4999 | Jaccard: 0.7298\n",
      "Epoch 3/6 |  val  | Loss: 1.5044 | Jaccard: 0.7290\n",
      "Epoch 4/6 | train | Loss: 1.3718 | Jaccard: 0.7471\n",
      "Epoch 4/6 |  val  | Loss: 1.5855 | Jaccard: 0.7230\n",
      "Epoch 5/6 | train | Loss: 1.2490 | Jaccard: 0.7646\n",
      "Epoch 5/6 |  val  | Loss: 1.6754 | Jaccard: 0.7265\n",
      "Epoch 6/6 | train | Loss: 1.1175 | Jaccard: 0.7840\n",
      "Epoch 6/6 |  val  | Loss: 1.8250 | Jaccard: 0.7147\n",
      "Fold: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-c17d34781fc6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders_dict, criterion, optimizer, num_epochs, filename)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "train_df['selected_text'] = train_df['selected_text'].astype(str)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "    if fold==1:\n",
    "        print(f'Fold: {fold}')\n",
    "\n",
    "        model = TweetModel()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "        criterion = loss_fn    \n",
    "        dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "\n",
    "        train_model(\n",
    "            model, \n",
    "            dataloaders_dict,\n",
    "            criterion, \n",
    "            optimizer, \n",
    "            num_epochs,\n",
    "            f'roberta_fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df['text'] = test_df['text'].astype(str)\n",
    "test_loader = get_test_loader(test_df)\n",
    "predictions = []\n",
    "models = []\n",
    "for fold in range(skf.n_splits):\n",
    "    model = TweetModel()\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "for data in test_loader:\n",
    "    ids = data['ids'].cuda()\n",
    "    masks = data['masks'].cuda()\n",
    "    tweet = data['tweet']\n",
    "    offsets = data['offsets'].numpy()\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, masks)\n",
    "            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    start_logits = np.mean(start_logits, axis=0)\n",
    "    end_logits = np.mean(end_logits, axis=0)\n",
    "    for i in range(len(ids)):    \n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        if start_pred > end_pred:\n",
    "            pred = tweet[i]\n",
    "        else:\n",
    "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "sub_df['selected_text'] = predictions\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
