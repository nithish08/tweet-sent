{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ALBERT a BERT based model for this task.\n",
    "\n",
    "More about ALBERT:\n",
    "\n",
    "The ALBERT model was proposed in ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT:\n",
    "\n",
    "> Splitting the embedding matrix into two smaller matrices\n",
    "> Using repeating layers split among groups\n",
    "\n",
    "[link](https://huggingface.co/transformers/model_doc/albert.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
    "from transformers import AlbertConfig\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set gpu id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> download the pretrained weights of alberta model\n",
    "\n",
    "**Run only once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "for col in ['text', 'selected_text']:\n",
    "    for df in [train_df, test_df]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col] = df[col].apply(lambda x: ' '.join(x.strip().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download pretrained model\n",
    "# tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "# model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')\n",
    "\n",
    "# if not os.path.exists('albert-base-v2'):\n",
    "#     os.mkdir('albert-base-v2')\n",
    "# tokenizer.save_pretrained('albert-base-v2')\n",
    "# model.save_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load weights for 'alberta-base-v2/pytorch_model.bin'. Make sure that:\n\n- 'alberta-base-v2/pytorch_model.bin' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'alberta-base-v2/pytorch_model.bin' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b7cc42df0bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'albert-base-v2/config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = AlbertForQuestionAnswering.from_pretrained(\n\u001b[0;32m----> 3\u001b[0;31m             'alberta-base-v2/pytorch_model.bin', config=config)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 )\n\u001b[0;32m--> 645\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load weights for 'alberta-base-v2/pytorch_model.bin'. Make sure that:\n\n- 'alberta-base-v2/pytorch_model.bin' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'alberta-base-v2/pytorch_model.bin' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.\n\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "config = AlbertConfig.from_pretrained('albert-base-v2/config.json')\n",
    "model = AlbertForQuestionAnswering.from_pretrained(\n",
    "            'alberta-base-v2/pytorch_model.bin', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I`d have responded, if I were going ['▁i', '`', 'd', '▁have', '▁responded', ',', '▁if', '▁i', '▁were', '▁going']\n",
      "Sooo SAD I will miss you here in San Diego!!! ['▁so', 'oo', '▁sad', '▁i', '▁will', '▁miss', '▁you', '▁here', '▁in', '▁san', '▁diego', '!!!']\n",
      "my boss is bullying me... ['▁my', '▁boss', '▁is', '▁bully', 'ing', '▁me', '.', '.', '.']\n",
      "what interview! leave me alone ['▁what', '▁interview', '!', '▁leave', '▁me', '▁alone']\n",
      "Sons of ****, why couldn`t they put them on the releases we already bought ['▁sons', '▁of', '▁****', ',', '▁why', '▁couldn', '`', 't', '▁they', '▁put', '▁them', '▁on', '▁the', '▁releases', '▁we', '▁already', '▁bought']\n",
      "http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth ['▁http', '://', 'www', '.', 'do', 'the', 'bou', 'ncy', '.', 'com', '/', 's', 'mf', '▁', '-', '▁some', '▁shame', 'less', '▁plug', 'ging', '▁for', '▁the', '▁best', '▁rangers', '▁forum', '▁on', '▁earth']\n",
      "2am feedings for the baby are fun when he is all smiles and coos ['▁2', 'am', '▁feeding', 's', '▁for', '▁the', '▁baby', '▁are', '▁fun', '▁when', '▁he', '▁is', '▁all', '▁smiles', '▁and', '▁coo', 's']\n",
      "Soooo high ['▁', 's', 'oooo', '▁high']\n",
      "Both of you ['▁both', '▁of', '▁you']\n",
      "Journey!? Wow... u just became cooler. hehe... (is that possible!?) ['▁journey', '!', '?', '▁wow', '.', '.', '.', '▁u', '▁just', '▁became', '▁cooler', '.', '▁he', 'he', '.', '.', '.', '▁', '(', 'is', '▁that', '▁possible', '!', '?', ')']\n",
      "as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff ['▁as', '▁much', '▁as', '▁i', '▁love', '▁to', '▁be', '▁hopeful', ',', '▁i', '▁reckon', '▁the', '▁chances', '▁are', '▁minimal', '▁=', 'p', '▁i', '`', 'm', '▁never', '▁gonna', '▁get', '▁my', '▁cake', '▁and', '▁stuff']\n",
      "I really really like the song Love Story by Taylor Swift ['▁i', '▁really', '▁really', '▁like', '▁the', '▁song', '▁love', '▁story', '▁by', '▁taylor', '▁swift']\n",
      "My Sharpie is running DANGERously low on ink ['▁my', '▁sharp', 'ie', '▁is', '▁running', '▁dangerously', '▁low', '▁on', '▁in', 'k']\n",
      "i want to go to music tonight but i lost my voice. ['▁i', '▁want', '▁to', '▁go', '▁to', '▁music', '▁tonight', '▁but', '▁i', '▁lost', '▁my', '▁voice', '.']\n",
      "test test from the LG enV2 ['▁test', '▁test', '▁from', '▁the', '▁', 'lg', '▁en', 'v', '2']\n",
      "Uh oh, I am sunburned ['▁uh', '▁oh', ',', '▁i', '▁am', '▁sun', 'burn', 'ed']\n",
      "S`ok, trying to plot alternatives as we speak *sigh* ['▁', 's', '`', 'ok', ',', '▁trying', '▁to', '▁plot', '▁alternative', 's', '▁as', '▁we', '▁speak', '▁*', 's', 'igh', '*']\n",
      "i`ve been sick for the past few days and thus, my hair looks wierd. if i didnt have a hat on it would look... http://tinyurl.com/mnf4kw ['▁i', '`', 've', '▁been', '▁sick', '▁for', '▁the', '▁past', '▁few', '▁days', '▁and', '▁thus', ',', '▁my', '▁hair', '▁looks', '▁wie', 'rd', '.', '▁if', '▁i', '▁didnt', '▁have', '▁a', '▁hat', '▁on', '▁it', '▁would', '▁look', '.', '.', '.', '▁http', '://', 'tin', 'yu', 'rl', '.', 'com', '/', 'm', 'nf', '4', 'kw']\n",
      "is back home now gonna miss every one ['▁is', '▁back', '▁home', '▁now', '▁gonna', '▁miss', '▁every', '▁one']\n",
      "Hes just not that into you ['▁he', 's', '▁just', '▁not', '▁that', '▁into', '▁you']\n"
     ]
    }
   ],
   "source": [
    "## let's see how it tokenizes text\n",
    "for idx in range(20):\n",
    "    text = train_df.text[idx]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    print(text, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how many unknown tokens are there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 14456/571005 -> 2 % unknown tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what's the maximum number of tokens for the padding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sents = [tokenizer.encode(sentence) for sentence in train_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(lst) for lst in encoded_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<cls> <question> <sep> <sentence-1-token> <sentence-1-token> <sentence-2-token> ...... <sep>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN  = 126 + 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=MAX_LEN):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'selected_text' in df\n",
    "        self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        ids, masks, tweet, offsets = self.get_input_data(row)\n",
    "        data['ids'] = ids\n",
    "        data['masks'] = masks\n",
    "        data['tweet'] = tweet\n",
    "        data['offsets'] = offsets\n",
    "        \n",
    "        if self.labeled:\n",
    "            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n",
    "            data['start_idx'] = start_idx\n",
    "            data['end_idx'] = end_idx\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_input_data(self, row):\n",
    "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "        encoding = self.tokenizer.encode(tweet)\n",
    "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
    "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
    "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "                \n",
    "        pad_len = self.max_len - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids += [1] * pad_len\n",
    "            offsets += [(0, 0)] * pad_len\n",
    "        \n",
    "        ids = torch.tensor(ids)\n",
    "        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "        offsets = torch.tensor(offsets)\n",
    "        \n",
    "        return ids, masks, tweet, offsets\n",
    "        \n",
    "    def get_target_idx(self, row, tweet, offsets):\n",
    "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "            if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "        start_idx = target_idx[0]\n",
    "        end_idx = target_idx[-1]\n",
    "        \n",
    "        return start_idx, end_idx\n",
    "        \n",
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TweetModel, self).__init__()\n",
    "        \n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            'roberta-base/config.json', output_hidden_states=True)    \n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            'roberta-base/pytorch_model.bin', config=config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(config.hidden_size, 2)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "         \n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)    \n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_selected_text(text, start_idx, end_idx, offsets):\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"  # add spaces for words in b/w\n",
    "    return selected_text\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n",
    "    ''' out model outputs 2 vectors\n",
    "    start logits -> probability that a token is the start\n",
    "    end logits   -> probability that a token is the ending'''\n",
    "    start_pred = np.argmax(start_logits)  # start pred\n",
    "    end_pred = np.argmax(end_logits)      # end pred\n",
    "    if start_pred > end_pred:\n",
    "        pred = text\n",
    "    else:\n",
    "        pred = get_selected_text(text, start_pred, end_pred, offsets)\n",
    "        \n",
    "    true = get_selected_text(text, start_idx, end_idx, offsets)\n",
    "    \n",
    "    return jaccard(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CUDA_AVAILABLE = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n",
    "    if CUDA_AVAILABLE:\n",
    "        model.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_jaccard = 0.0\n",
    "            \n",
    "            for data in (dataloaders_dict[phase]):\n",
    "                ids = data['ids'].cuda()\n",
    "                masks = data['masks'].cuda()\n",
    "                tweet = data['tweet']\n",
    "                offsets = data['offsets'].numpy()\n",
    "                start_idx = data['start_idx'].cuda()\n",
    "                end_idx = data['end_idx'].cuda()\n",
    "\n",
    "                if CUDA_AVAILABLE:\n",
    "                    ids = ids.cuda()\n",
    "                    masks = masks.cuda()\n",
    "                    start_idx = start_idx.cuda()\n",
    "                    end_idx = end_idx.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    start_logits, end_logits = model(ids, masks)\n",
    "\n",
    "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * len(ids)\n",
    "                    \n",
    "                    start_idx = start_idx.cpu().detach().numpy()\n",
    "                    end_idx = end_idx.cpu().detach().numpy()\n",
    "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n",
    "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n",
    "                    \n",
    "                    for i in range(len(ids)):                        \n",
    "                        jaccard_score = compute_jaccard_score(\n",
    "                            tweet[i],\n",
    "                            start_idx[i],\n",
    "                            end_idx[i],\n",
    "                            start_logits[i], \n",
    "                            end_logits[i], \n",
    "                            offsets[i])\n",
    "                        epoch_jaccard += jaccard_score\n",
    "                    \n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n",
    "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n",
    "    \n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "batch_size = 64\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'merges_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-390c2307ab1f>\u001b[0m in \u001b[0;36mget_train_val_loaders\u001b[0;34m(df, train_idx, val_idx, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mTweetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-390c2307ab1f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, max_len)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mmerges_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roberta-base/merges.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             wordpieces_prefix=' ')\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'merges_file'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n",
    "    print(f'Fold: {fold}')\n",
    "\n",
    "    model = TweetModel()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "    criterion = loss_fn    \n",
    "    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n",
    "\n",
    "    train_model(\n",
    "        model, \n",
    "        dataloaders_dict,\n",
    "        criterion, \n",
    "        optimizer, \n",
    "        num_epochs,\n",
    "        f'roberta_char_level_fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test_df['text'] = test_df['text'].astype(str)\n",
    "test_loader = get_test_loader(test_df)\n",
    "predictions = []\n",
    "models = []\n",
    "for fold in range(skf.n_splits):\n",
    "    model = TweetModel()\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "for data in test_loader:\n",
    "    ids = data['ids'].cuda()\n",
    "    masks = data['masks'].cuda()\n",
    "    tweet = data['tweet']\n",
    "    offsets = data['offsets'].numpy()\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, masks)\n",
    "            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    start_logits = np.mean(start_logits, axis=0)\n",
    "    end_logits = np.mean(end_logits, axis=0)\n",
    "    for i in range(len(ids)):    \n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        if start_pred > end_pred:\n",
    "            pred = tweet[i]\n",
    "        else:\n",
    "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "sub_df['selected_text'] = predictions\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
