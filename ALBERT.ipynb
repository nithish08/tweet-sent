{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this notebook:\n",
    "\n",
    "1. Load data and do preprocessing for ALBERT model.\n",
    "2. Train the ALBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about ALBERT:\n",
    "\n",
    "The ALBERT model was proposed in ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT:\n",
    "\n",
    "1. Splitting the embedding matrix into two smaller matrices.\n",
    "1. Using repeating layers split among groups.\n",
    "\n",
    "[link](https://huggingface.co/transformers/model_doc/albert.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AlbertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set gpu id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> download the pretrained weights of alberta model\n",
    "\n",
    "**Run only once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "for col in ['text', 'selected_text']:\n",
    "    for df in [train, test]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col] = df[col].apply(lambda x: ' '.join(x.strip().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download pretrained model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save pretrained model to local directory\n",
    "\n",
    " for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('pretrained_models/albert-base-v2'):\n",
    "#     os.mkdir('pretrained_models/albert-base-v2')\n",
    "\n",
    "# tokenizer.save_pretrained('pretrained_models/albert-base-v2')\n",
    "# model.save_pretrained('pretrained_models/albert-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model from local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how many unknown tokens are there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 14456/571005 -> 2 % unknown tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what's the maximum number of tokens for the padding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_id(question_context_encoded,\n",
    "                    answer_encoded):\n",
    "    '''\n",
    "    Returns Start and end id of tokens of answer in the\n",
    "    given question context vector.\n",
    "    These indices are used as targets for ALBERT QA model.\n",
    "    \n",
    "    Input Args:\n",
    "    \n",
    "        question_context_encoded -> tensor\n",
    "                                    shape = (1, MAX_LEN)\n",
    "                                    \n",
    "        answer_encoded           -> tensor\n",
    "                                    shape = (1, ANSWER_LEN)\n",
    "    \n",
    "    return :\n",
    "    \n",
    "    start_id   ->  tensor with one value\n",
    "    end_id   ->  tensor with one value\n",
    "    '''\n",
    "    \n",
    "    answer_len = answer_encoded.shape[1]\n",
    "    for idx in range(question_context_encoded.shape[1]):\n",
    "        if question_context_encoded[0,idx] == answer_encoded[0,0] and \\\n",
    "        torch.equal(question_context_encoded[0, idx:idx+answer_len].flatten()\n",
    "                    , answer_encoded.flatten()):\n",
    "            return torch.LongTensor([idx]).reshape(1,-1),  \\\n",
    "                        torch.LongTensor([idx+answer_len-1]).reshape(1,-1)\n",
    "    \n",
    "    return torch.LongTensor([0]).reshape(1,1), torch.LongTensor([0]).reshape(1,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=MAX_LEN):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'selected_text' in df.columns\n",
    "        self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data = {}\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        context  = row.text\n",
    "        question = row.sentiment\n",
    "        \n",
    "        qc_encoded = tokenizer.encode_plus(question,context,\n",
    "                                                         max_length=MAX_LEN,\n",
    "                pad_to_max_length=True, return_tensors='pt')\n",
    "        \n",
    "        for key,value in qc_encoded.items(): # question context encoded tensors\n",
    "            data[key] = value.reshape(-1)\n",
    "            \n",
    "        if self.labeled:\n",
    "            answer   = row.selected_text\n",
    "            answer_encoded = tokenizer.encode_plus(answer,return_tensors='pt',\n",
    "                     add_special_tokens=False)['input_ids']\n",
    "            start_idx, end_idx = get_start_end_id(qc_encoded['input_ids'],\n",
    "                                                 answer_encoded)\n",
    "            data['start_idx'] = start_idx.reshape(-1)\n",
    "            data['end_idx']   = end_idx.reshape(-1)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(TweetModel, self).__init__()\n",
    "        \n",
    "        self.tokenizer = hparams.tokenizer\n",
    "        self.train_df  = hparams.train_df\n",
    "        self.val_df    = hparams.val_df\n",
    "        self.lr        = hparams.lr\n",
    "        self.batch_size= hparams.batch_size\n",
    "        self.n_workers = hparams.n_workers\n",
    "        \n",
    "        # load albert model\n",
    "        config         = AlbertConfig.from_pretrained(\n",
    "                                    'pretrained_models/albert-base-v2/config.json')\n",
    "        self.model     = AlbertForQuestionAnswering.from_pretrained(\n",
    "                            'pretrained_models/albert-base-v2/pytorch_model.bin', config=config)\n",
    "\n",
    "        # tensorboard purposes\n",
    "        self.train_recorder = 0\n",
    "        self.val_recorder   = 0\n",
    "        \n",
    "        # log hparams\n",
    "#         self.logger.experiment.add_hparams(hparam_dict={'lr':lr},metric_dict={})\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids,\n",
    "               start_positions=None, end_positions=None):\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if start_positions is None:\n",
    "            input_ids      = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            start_logits, end_logits = self.model.forward(input_ids, attention_mask,\n",
    "                                                         token_type_ids)\n",
    "        else: \n",
    "            input_ids      = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            start_positions= start_positions.to(device)\n",
    "            end_positions  = end_positions.to(device)\n",
    "            \n",
    "            loss, start_logits, end_logits = self.model.forward(input_ids = input_ids,\n",
    "                                                                attention_mask = attention_mask,\n",
    "                                                                token_type_ids = token_type_ids, \n",
    "                                                                start_positions= start_positions,\n",
    "                                                                end_positions  = end_positions)\n",
    "        return loss, start_logits, end_logits\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, start_logits, end_logits = self.forward(input_ids=batch['input_ids'], \n",
    "                                       attention_mask=batch['attention_mask'], \n",
    "                                       token_type_ids=batch['token_type_ids'],\n",
    "                                       start_positions= batch['start_idx'],\n",
    "                                       end_positions =  batch['end_idx'])\n",
    "        \n",
    "        self.logger.experiment.add_scalar('loss/train',\n",
    "                                          loss.item(),\n",
    "                                          global_step=self.train_recorder)\n",
    "        self.train_recorder += 1  # for tensorboard purposes\n",
    "        \n",
    "        pb = {'train_loss':loss}\n",
    "        return {'loss': loss, 'progress_bar':pb}\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW([p for p in self.parameters() if p.requires_grad],\n",
    "                                 lr=self.lr, betas=(0.9, 0.999))\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        loss, start_logits, end_logits = self.forward(input_ids=batch['input_ids'], \n",
    "                                       attention_mask=batch['attention_mask'], \n",
    "                                       token_type_ids=batch['token_type_ids'],\n",
    "                                       start_positions= batch['start_idx'],\n",
    "                                       end_positions =  batch['end_idx'])\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        preds = get_batch_predictions(self.tokenizer,\n",
    "                                      input_ids, start_logits, end_logits)\n",
    "        tars  = get_target_text(self.tokenizer, input_ids,\n",
    "                                batch['start_idx'], batch['end_idx'])\n",
    "        \n",
    "        jaccard_score = average_jaccard(tars, preds)\n",
    "        pb = {'val_loss':loss}\n",
    "        return {'val_loss':loss, 'val_jaccard':torch.tensor([jaccard_score]),\n",
    "               'progress_bar':pb}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss    = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_jaccard = torch.stack([x['val_jaccard'] for x in outputs]).mean()\n",
    "        \n",
    "        self.logger.experiment.add_scalar('loss/val', avg_loss.item(), global_step=self.val_recorder)\n",
    "        self.logger.experiment.add_scalar('jaccard/val',avg_jaccard, global_step=self.val_recorder)\n",
    "        self.val_recorder += 1\n",
    "\n",
    "        return {'val_loss':avg_loss}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                                        TweetDataset(self.val_df), \n",
    "                                        batch_size=self.batch_size, \n",
    "                                        shuffle=False, \n",
    "                                        num_workers=self.n_workers)\n",
    "        return val_loader\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                                        TweetDataset(self.train_df), \n",
    "                                        batch_size=self.batch_size, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=self.n_workers)\n",
    "        return train_loader\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_predictions(tokenizer, input_ids, start_logits, end_logits):\n",
    "    '''\n",
    "    \n",
    "    Input Args:\n",
    "    \n",
    "    input_ids    -> (batch_size, max_len)\n",
    "    start_logits -> (batch_size, start_scores)\n",
    "    end_logits   -> (batch_size, end_scores)\n",
    "    \n",
    "    Output:\n",
    "    List[selectec_text]\n",
    "    '''\n",
    "    \n",
    "    input_ids = input_ids.cpu().detach().numpy()\n",
    "    start_logits = start_logits.cpu().detach().numpy()\n",
    "    end_logits = end_logits.cpu().detach().numpy()\n",
    "    \n",
    "    preds = []\n",
    "    for row_id in range(input_ids.shape[0]):\n",
    "        \n",
    "        ids = input_ids[row_id,:]\n",
    "        start = np.argmax(start_logits[row_id,:])\n",
    "        end = np.argmax(end_logits[row_id,:])\n",
    "        \n",
    "        if start>end:\n",
    "            sel_ids = ids  # whole list\n",
    "            \n",
    "        else:\n",
    "            sel_ids = ids[start:end+1]\n",
    "            \n",
    "        text = tokenizer.convert_tokens_to_string(\n",
    "                        tokenizer.convert_ids_to_tokens(sel_ids))\n",
    "        preds.append(text)\n",
    "        \n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_target_text(tokenizer, input_ids, starts, ends):\n",
    "    '''gets the target text given tokenizer \n",
    "    input ids, starts and ends'''\n",
    "    input_ids = input_ids.cpu().detach().numpy()\n",
    "    starts = starts.cpu().detach().numpy().flatten()\n",
    "    ends = ends.cpu().detach().numpy().flatten()\n",
    "    \n",
    "    tars = []\n",
    "    for row_id in range(input_ids.shape[0]):\n",
    "        ids = input_ids[row_id,:]\n",
    "        start, end = starts[row_id], ends[row_id]\n",
    "        text = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(ids[start:end+1]))\n",
    "        tars.append(text)\n",
    "    \n",
    "    return tars\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def average_jaccard(targets, predictions):\n",
    "    '''get average jaccard for multiple predictions'''\n",
    "    total_score = 0\n",
    "    \n",
    "    for t, p in zip(targets, predictions):\n",
    "        sample_score = jaccard(t, p)\n",
    "        total_score += sample_score\n",
    "    \n",
    "    return total_score/len(targets)  # average\n",
    "\n",
    "def get_total_params(model):\n",
    "    s = 0\n",
    "    for param in model.parameters():\n",
    "        s += param.numel()\n",
    "    return s\n",
    "\n",
    "def get_trainable_params(model):\n",
    "    s = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            s += param.numel()\n",
    "    return s\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name)\n",
    "\n",
    "def finetune_model(model, last_n):\n",
    "    '''freezes all the layers in the model except last_n number of layers\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    model  :  pytorch model\n",
    "    last_n    :  number of last layers to unfreeze\n",
    "    '''\n",
    "    \n",
    "    total_layers = len(list(model.parameters()))\n",
    "    \n",
    "    for enum, param in enumerate(model.parameters()):\n",
    "        param.requires_grad = False\n",
    "        if enum + last_n >= total_layers:\n",
    "            param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)    \n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "batch_size = 64\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('pretrained_models/albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "CPU times: user 28.9 ms, sys: 345 µs, total: 29.3 ms\n",
      "Wall time: 28.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train, train.sentiment), start=1): \n",
    "    print(f'Fold: {fold}')\n",
    "    train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(tokenizer=tokenizer,train_df=train_df, val_df=val_df,\n",
    "                        lr=3e-5, batch_size=64,n_workers=4)\n",
    "\n",
    "hparams = Namespace(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "\n",
      "   | Name                                                                             | Type                       | Params\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "0  | model                                                                            | AlbertForQuestionAnswering | 11 M  \n",
      "1  | model.albert                                                                     | AlbertModel                | 11 M  \n",
      "2  | model.albert.embeddings                                                          | AlbertEmbeddings           | 3 M   \n",
      "3  | model.albert.embeddings.word_embeddings                                          | Embedding                  | 3 M   \n",
      "4  | model.albert.embeddings.position_embeddings                                      | Embedding                  | 65 K  \n",
      "5  | model.albert.embeddings.token_type_embeddings                                    | Embedding                  | 256   \n",
      "6  | model.albert.embeddings.LayerNorm                                                | LayerNorm                  | 256   \n",
      "7  | model.albert.embeddings.dropout                                                  | Dropout                    | 0     \n",
      "8  | model.albert.encoder                                                             | AlbertTransformer          | 7 M   \n",
      "9  | model.albert.encoder.embedding_hidden_mapping_in                                 | Linear                     | 99 K  \n",
      "10 | model.albert.encoder.albert_layer_groups                                         | ModuleList                 | 7 M   \n",
      "11 | model.albert.encoder.albert_layer_groups.0                                       | AlbertLayerGroup           | 7 M   \n",
      "12 | model.albert.encoder.albert_layer_groups.0.albert_layers                         | ModuleList                 | 7 M   \n",
      "13 | model.albert.encoder.albert_layer_groups.0.albert_layers.0                       | AlbertLayer                | 7 M   \n",
      "14 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm | LayerNorm                  | 1 K   \n",
      "15 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention             | AlbertAttention            | 2 M   \n",
      "16 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query       | Linear                     | 590 K \n",
      "17 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key         | Linear                     | 590 K \n",
      "18 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value       | Linear                     | 590 K \n",
      "19 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dropout     | Dropout                    | 0     \n",
      "20 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense       | Linear                     | 590 K \n",
      "21 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm   | LayerNorm                  | 1 K   \n",
      "22 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn                   | Linear                     | 2 M   \n",
      "23 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output            | Linear                     | 2 M   \n",
      "24 | model.albert.pooler                                                              | Linear                     | 590 K \n",
      "25 | model.albert.pooler_activation                                                   | Tanh                       | 0     \n",
      "26 | model.qa_outputs                                                                 | Linear                     | 1 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2341d4a4c04f509b12161ac259bbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Finding best initial lr', style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_model = TweetModel(hparams)\n",
    "finetune_model(tweet_model, 15)\n",
    "\n",
    "name = 'albert'\n",
    "logger = TensorBoardLogger(\n",
    "                save_dir='ts-logs',\n",
    "                name = name\n",
    "            )\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.mkdir('saved_models')\n",
    "if not os.path.exists(f'saved_models/{name}'):\n",
    "    os.mkdir(f'saved_models/{name}')\n",
    "checkpoint_callback = ModelCheckpoint(f'saved_models/{name}', save_top_k=2)\n",
    "early_stopping = EarlyStopping(monitor='val_jaccard')\n",
    "\n",
    "tweet_model.cuda()\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     checkpoint_callback=checkpoint_callback,\n",
    "                     logger=logger,\n",
    "                    early_stop_callback=early_stopping,\n",
    "                    auto_lr_find=True)\n",
    "\n",
    "\n",
    "trainer.fit(tweet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams = dict(tokenizer=tokenizer,\n",
    "           train_df=train_df, \n",
    "           val_df=val_df,\n",
    "           lr=3e-5, batch_size=64,\n",
    "           n_workers=4)\n",
    "hparams = Namespace(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-12e8c6927de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m tweet_model.load_from_checkpoint('saved_models/albert/epoch=4.ckpt',\n\u001b[0;32m----> 2\u001b[0;31m                                 hparams_file=hparams)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, tags_csv, hparam_overrides, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m                 \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hparams_from_tags_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "tweet_model.load_from_checkpoint('saved_models/albert/epoch=4.ckpt',\n",
    "                                hparams_file=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train, train.sentiment), start=1): \n",
    "    print(f'Fold: {fold}')\n",
    "\n",
    "    model = TweetModel()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "    criterion = loss_fn    \n",
    "    dataloaders_dict = get_train_val_loaders(train, train_idx, val_idx, batch_size)\n",
    "\n",
    "    train_model(\n",
    "        model, \n",
    "        dataloaders_dict,\n",
    "        criterion, \n",
    "        optimizer, \n",
    "        num_epochs,\n",
    "        f'roberta_char_level_fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test['text'] = test['text'].astype(str)\n",
    "test_loader = get_test_loader(test)\n",
    "predictions = []\n",
    "models = []\n",
    "for fold in range(skf.n_splits):\n",
    "    model = TweetModel()\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "for data in test_loader:\n",
    "    ids = data['ids'].cuda()\n",
    "    masks = data['masks'].cuda()\n",
    "    tweet = data['tweet']\n",
    "    offsets = data['offsets'].numpy()\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, masks)\n",
    "            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    start_logits = np.mean(start_logits, axis=0)\n",
    "    end_logits = np.mean(end_logits, axis=0)\n",
    "    for i in range(len(ids)):    \n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        if start_pred > end_pred:\n",
    "            pred = tweet[i]\n",
    "        else:\n",
    "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "sub_df['selected_text'] = predictions\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
