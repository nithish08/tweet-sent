{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this notebook:\n",
    "\n",
    "1. Load data and do preprocessing for ALBERT model.\n",
    "2. Train the ALBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about ALBERT:\n",
    "\n",
    "The ALBERT model was proposed in ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT:\n",
    "\n",
    "1. Splitting the embedding matrix into two smaller matrices.\n",
    "1. Using repeating layers split among groups.\n",
    "\n",
    "[link](https://huggingface.co/transformers/model_doc/albert.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import torch \n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AlbertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tokenizers\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set gpu id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> download the pretrained weights of alberta model\n",
    "\n",
    "**Run only once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "for col in ['text', 'selected_text']:\n",
    "    for df in [train, test]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col] = df[col].apply(lambda x: ' '.join(x.strip().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download pretrained model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"elgeish/cs224n-squad2.0-albert-base-v2\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"elgeish/cs224n-squad2.0-albert-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save pretrained model to local directory\n",
    "\n",
    " for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('pretrained_models/albert-qa'):\n",
    "#     os.mkdir('pretrained_models/albert-qa')\n",
    "\n",
    "# tokenizer.save_pretrained('pretrained_models/albert-qa/')\n",
    "# model.save_pretrained('pretrained_models/albert-qa/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model from local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I`d have responded, if I were going ['▁i', '`', 'd', '▁have', '▁responded', ',', '▁if', '▁i', '▁were', '▁going']\n",
      "Sooo SAD I will miss you here in San Diego!!! ['▁so', 'oo', '▁sad', '▁i', '▁will', '▁miss', '▁you', '▁here', '▁in', '▁san', '▁diego', '!!!']\n",
      "my boss is bullying me... ['▁my', '▁boss', '▁is', '▁bully', 'ing', '▁me', '.', '.', '.']\n",
      "what interview! leave me alone ['▁what', '▁interview', '!', '▁leave', '▁me', '▁alone']\n",
      "Sons of ****, why couldn`t they put them on the releases we already bought ['▁sons', '▁of', '▁****', ',', '▁why', '▁couldn', '`', 't', '▁they', '▁put', '▁them', '▁on', '▁the', '▁releases', '▁we', '▁already', '▁bought']\n",
      "http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth ['▁http', '://', 'www', '.', 'do', 'the', 'bou', 'ncy', '.', 'com', '/', 's', 'mf', '▁', '-', '▁some', '▁shame', 'less', '▁plug', 'ging', '▁for', '▁the', '▁best', '▁rangers', '▁forum', '▁on', '▁earth']\n",
      "2am feedings for the baby are fun when he is all smiles and coos ['▁2', 'am', '▁feeding', 's', '▁for', '▁the', '▁baby', '▁are', '▁fun', '▁when', '▁he', '▁is', '▁all', '▁smiles', '▁and', '▁coo', 's']\n",
      "Soooo high ['▁', 's', 'oooo', '▁high']\n",
      "Both of you ['▁both', '▁of', '▁you']\n",
      "Journey!? Wow... u just became cooler. hehe... (is that possible!?) ['▁journey', '!', '?', '▁wow', '.', '.', '.', '▁u', '▁just', '▁became', '▁cooler', '.', '▁he', 'he', '.', '.', '.', '▁', '(', 'is', '▁that', '▁possible', '!', '?', ')']\n",
      "as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff ['▁as', '▁much', '▁as', '▁i', '▁love', '▁to', '▁be', '▁hopeful', ',', '▁i', '▁reckon', '▁the', '▁chances', '▁are', '▁minimal', '▁=', 'p', '▁i', '`', 'm', '▁never', '▁gonna', '▁get', '▁my', '▁cake', '▁and', '▁stuff']\n",
      "I really really like the song Love Story by Taylor Swift ['▁i', '▁really', '▁really', '▁like', '▁the', '▁song', '▁love', '▁story', '▁by', '▁taylor', '▁swift']\n",
      "My Sharpie is running DANGERously low on ink ['▁my', '▁sharp', 'ie', '▁is', '▁running', '▁dangerously', '▁low', '▁on', '▁in', 'k']\n",
      "i want to go to music tonight but i lost my voice. ['▁i', '▁want', '▁to', '▁go', '▁to', '▁music', '▁tonight', '▁but', '▁i', '▁lost', '▁my', '▁voice', '.']\n",
      "test test from the LG enV2 ['▁test', '▁test', '▁from', '▁the', '▁', 'lg', '▁en', 'v', '2']\n",
      "Uh oh, I am sunburned ['▁uh', '▁oh', ',', '▁i', '▁am', '▁sun', 'burn', 'ed']\n",
      "S`ok, trying to plot alternatives as we speak *sigh* ['▁', 's', '`', 'ok', ',', '▁trying', '▁to', '▁plot', '▁alternative', 's', '▁as', '▁we', '▁speak', '▁*', 's', 'igh', '*']\n",
      "i`ve been sick for the past few days and thus, my hair looks wierd. if i didnt have a hat on it would look... http://tinyurl.com/mnf4kw ['▁i', '`', 've', '▁been', '▁sick', '▁for', '▁the', '▁past', '▁few', '▁days', '▁and', '▁thus', ',', '▁my', '▁hair', '▁looks', '▁wie', 'rd', '.', '▁if', '▁i', '▁didnt', '▁have', '▁a', '▁hat', '▁on', '▁it', '▁would', '▁look', '.', '.', '.', '▁http', '://', 'tin', 'yu', 'rl', '.', 'com', '/', 'm', 'nf', '4', 'kw']\n",
      "is back home now gonna miss every one ['▁is', '▁back', '▁home', '▁now', '▁gonna', '▁miss', '▁every', '▁one']\n",
      "Hes just not that into you ['▁he', 's', '▁just', '▁not', '▁that', '▁into', '▁you']\n"
     ]
    }
   ],
   "source": [
    "## let's see how it tokenizes text\n",
    "for idx in range(20):\n",
    "    text = train.text[idx]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    print(text, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how many unknown tokens are there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 14456/571005 -> 2 % unknown tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what's the maximum number of tokens for the padding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_sents = [tokenizer.encode(sentence) for sentence in train.text]\n",
    "train_lens = [len(lst) for lst in train_encoded_sents]\n",
    "train_max_len = max(train_lens)\n",
    "\n",
    "test_encoded_sents = [tokenizer.encode(sentence) for sentence in test.text]\n",
    "test_lens = [len(lst) for lst in test_encoded_sents]\n",
    "test_max_len = max(test_lens)\n",
    "\n",
    "MAX_LEN = max(train_max_len, test_max_len)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## format of question context into ALBERT model looks like this ->\n",
    "<cls> <question> <sep> <sentence-1-token> <sentence-1-token> <sentence-2-token> ...... <sep>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN  = MAX_LEN + 4 # we need to add for the special tokens and the question (one word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_id(question_context_encoded,\n",
    "                    answer_encoded):\n",
    "    '''\n",
    "    Returns Start and end id of tokens of answer in the\n",
    "    given question context vector.\n",
    "    These indices are used as targets for ALBERT QA model.\n",
    "    \n",
    "    Input Args:\n",
    "    \n",
    "        question_context_encoded -> tensor\n",
    "                                    shape = (1, MAX_LEN)\n",
    "                                    \n",
    "        answer_encoded           -> tensor\n",
    "                                    shape = (1, ANSWER_LEN)\n",
    "    \n",
    "    return :\n",
    "    \n",
    "    start_id   ->  tensor with one value\n",
    "    end_id   ->  tensor with one value\n",
    "    '''\n",
    "    \n",
    "    answer_len = answer_encoded.shape[1]\n",
    "    for idx in range(question_context_encoded.shape[1]):\n",
    "        if question_context_encoded[0,idx] == answer_encoded[0,0] and \\\n",
    "        torch.equal(question_context_encoded[0, idx:idx+answer_len].flatten()\n",
    "                    , answer_encoded.flatten()):\n",
    "            return torch.LongTensor([idx]).reshape(1,-1),  \\\n",
    "                        torch.LongTensor([idx+answer_len-1]).reshape(1,-1)\n",
    "    \n",
    "    return torch.LongTensor([0]).reshape(1,1), torch.LongTensor([0]).reshape(1,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=MAX_LEN):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.labeled = 'selected_text' in df.columns\n",
    "        self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data = {}\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        context  = row.text\n",
    "        question = row.sentiment\n",
    "        \n",
    "        qc_encoded = tokenizer.encode_plus(question,context,\n",
    "                                                         max_length=MAX_LEN,\n",
    "                pad_to_max_length=True, return_tensors='pt')\n",
    "        \n",
    "        for key,value in qc_encoded.items(): # question context encoded tensors\n",
    "            data[key] = value.reshape(-1).to(device)\n",
    "            \n",
    "        if self.labeled:\n",
    "            answer   = row.selected_text\n",
    "            answer_encoded = tokenizer.encode_plus(answer,return_tensors='pt',\n",
    "                     add_special_tokens=False)['input_ids']\n",
    "            start_idx, end_idx = get_start_end_id(qc_encoded['input_ids'],\n",
    "                                                 answer_encoded)\n",
    "            data['start_idx'] = start_idx.reshape(-1).to(device)\n",
    "            data['end_idx']   = end_idx.reshape(-1).to(device)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n",
    "    train = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(train), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(val_df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)\n",
    "\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    return dataloaders_dict\n",
    "\n",
    "def get_test_loader(df, batch_size=32):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        TweetDataset(df), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2)    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, train_df, val_df,\n",
    "                 lr, batch_size, n_workers=2):\n",
    "        super(TweetModel, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_df  = train_df\n",
    "        self.val_df    = val_df\n",
    "        self.lr = lr\n",
    "        self.batch_size= batch_size\n",
    "        self.n_workers = n_workers\n",
    "        config         = AlbertConfig.from_pretrained(\n",
    "                                    'pretrained_models/albert-qa/config.json')\n",
    "        self.model     = AlbertForQuestionAnswering.from_pretrained(\n",
    "                            'pretrained_models/albert-qa/pytorch_model.bin', config=config)\n",
    "        self.lr        = lr\n",
    "        self.batch_size= batch_size\n",
    "        \n",
    "        # tensorboard purposes\n",
    "        self.train_recorder = 0\n",
    "        self.val_recorder   = 0\n",
    "        \n",
    "        # log hparams\n",
    "        writer.add_hparams(hparam_dict={'lr':lr},metric_dict={})\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids,\n",
    "               start_positions=None, end_positions=None):\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if start_positions is None:\n",
    "            start_logits, end_logits = self.model.forward(input_ids, attention_mask,\n",
    "                                                         token_type_ids)\n",
    "        else: \n",
    "            \n",
    "            loss, start_logits, end_logits = self.model.forward(input_ids = input_ids,\n",
    "                                                                attention_mask = attention_mask,\n",
    "                                                                token_type_ids = token_type_ids, \n",
    "                                                                start_positions= start_positions,\n",
    "                                                                end_positions  = end_positions)\n",
    "        return start_logits, end_logits\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, start_logits, end_logits = self.model(input_ids=batch['input_ids'], \n",
    "                                       attention_mask=batch['attention_mask'], \n",
    "                                       token_type_ids=batch['token_type_ids'],\n",
    "                                       start_positions= batch['start_idx'],\n",
    "                                       end_positions =  batch['end_idx'])\n",
    "        \n",
    "        self.logger.experiment.add_scalar('loss/train',\n",
    "                                          loss.item(),\n",
    "                                          global_step=self.train_recorder)\n",
    "        self.train_recorder += 1  # for tensorboard purposes\n",
    "        \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW([p for p in self.parameters() if p.requires_grad],\n",
    "                                 lr=self.lr, betas=(0.9, 0.999))\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        loss, start_logits, end_logits = self.model(input_ids=batch['input_ids'], \n",
    "                                       attention_mask=batch['attention_mask'], \n",
    "                                       token_type_ids=batch['token_type_ids'],\n",
    "                                       start_positions= batch['start_idx'],\n",
    "                                       end_positions =  batch['end_idx'])\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        preds = get_batch_predictions(self.tokenizer,input_ids, start_logits, end_logits)\n",
    "        tars  = get_target_text(tokenizer, batch['input_ids'], \n",
    "                                batch['start_idx'], batch['end_idx'])\n",
    "        \n",
    "        jaccard_score = average_jaccard(tars, preds)\n",
    "        return {'val_loss':loss, 'val_jaccard':torch.tensor([jaccard_score])}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss    = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_jaccard = torch.stack([x['val_jaccard'] for x in outputs]).mean()\n",
    "        \n",
    "        self.logger.experiment.add_scalar('loss/val', avg_loss.item(), global_step=self.val_recorder)\n",
    "        self.logger.experiment.add_scalar('jaccard/val',avg_jaccard, global_step=self.val_recorder)\n",
    "        self.val_recorder += 1\n",
    "        return {'val_loss':avg_loss}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "                                        TweetDataset(self.val_df), \n",
    "                                        batch_size=self.batch_size, \n",
    "                                        shuffle=False, \n",
    "                                        num_workers=self.n_workers)\n",
    "        return val_loader\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                                        TweetDataset(self.train_df), \n",
    "                                        batch_size=self.batch_size, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=self.n_workers)\n",
    "        return train_loader\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_predictions(tokenizer, input_ids, start_logits, end_logits):\n",
    "    '''\n",
    "    \n",
    "    Input Args:\n",
    "    \n",
    "    input_ids    -> (batch_size, max_len)\n",
    "    start_logits -> (batch_size, start_scores)\n",
    "    end_logits   -> (batch_size, end_scores)\n",
    "    \n",
    "    Output:\n",
    "    List[selectec_text]\n",
    "    '''\n",
    "    \n",
    "    input_ids = input_ids.cpu().detach().numpy()\n",
    "    start_logits = start_logits.cpu().detach().numpy()\n",
    "    end_logits = end_logits.cpu().detach().numpy()\n",
    "    \n",
    "    preds = []\n",
    "    for row_id in range(input_ids.shape[0]):\n",
    "        \n",
    "        ids = input_ids[row_id,:]\n",
    "        start = np.argmax(start_logits[row_id,:])\n",
    "        end = np.argmax(end_logits[row_id,:])\n",
    "        \n",
    "        if start>end:\n",
    "            preds.append(text)\n",
    "            \n",
    "        else:\n",
    "            sel_ids = ids[start:end+1]\n",
    "            text = tokenizer.convert_tokens_to_string(\n",
    "                            tokenizer.convert_ids_to_tokens(sel_ids))\n",
    "            preds.append(text)\n",
    "        \n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_target_text(tokenizer, input_ids, starts, ends):\n",
    "    '''gets the target text given tokenizer \n",
    "    input ids, starts and ends'''\n",
    "    input_ids = input_ids.cpu().detach().numpy()\n",
    "    starts = starts.cpu().detach().numpy().flatten()\n",
    "    ends = ends.cpu().detach().numpy().flatten()\n",
    "    \n",
    "    tars = []\n",
    "    for row_id in range(input_ids.shape[0]):\n",
    "        ids = input_ids[row_id,:]\n",
    "        start, end = starts[row_id], ends[row_id]\n",
    "        text = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(ids[start:end+1]))\n",
    "        tars.append(text)\n",
    "    \n",
    "    return tars\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def average_jaccard(targets, predictions):\n",
    "    '''get average jaccard for multiple predictions'''\n",
    "    total_score = 0\n",
    "    \n",
    "    for t, p in zip(targets, predictions):\n",
    "        sample_score = jaccard(t, p)\n",
    "        total_score += sample_score\n",
    "    \n",
    "    return total_score/len(targets)  # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    start_loss = ce_loss(start_logits, start_positions)\n",
    "    end_loss = ce_loss(end_logits, end_positions)    \n",
    "    total_loss = start_loss + end_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "batch_size = 64\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('pretrained_models/albert-qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "CPU times: user 44.2 ms, sys: 3.87 ms, total: 48 ms\n",
      "Wall time: 53.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train, train.sentiment), start=1): \n",
    "    print(f'Fold: {fold}')\n",
    "    train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "\n",
      "   | Name                                                                             | Type                       | Params\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "0  | model                                                                            | AlbertForQuestionAnswering | 11 M  \n",
      "1  | model.albert                                                                     | AlbertModel                | 11 M  \n",
      "2  | model.albert.embeddings                                                          | AlbertEmbeddings           | 3 M   \n",
      "3  | model.albert.embeddings.word_embeddings                                          | Embedding                  | 3 M   \n",
      "4  | model.albert.embeddings.position_embeddings                                      | Embedding                  | 65 K  \n",
      "5  | model.albert.embeddings.token_type_embeddings                                    | Embedding                  | 256   \n",
      "6  | model.albert.embeddings.LayerNorm                                                | LayerNorm                  | 256   \n",
      "7  | model.albert.embeddings.dropout                                                  | Dropout                    | 0     \n",
      "8  | model.albert.encoder                                                             | AlbertTransformer          | 7 M   \n",
      "9  | model.albert.encoder.embedding_hidden_mapping_in                                 | Linear                     | 99 K  \n",
      "10 | model.albert.encoder.albert_layer_groups                                         | ModuleList                 | 7 M   \n",
      "11 | model.albert.encoder.albert_layer_groups.0                                       | AlbertLayerGroup           | 7 M   \n",
      "12 | model.albert.encoder.albert_layer_groups.0.albert_layers                         | ModuleList                 | 7 M   \n",
      "13 | model.albert.encoder.albert_layer_groups.0.albert_layers.0                       | AlbertLayer                | 7 M   \n",
      "14 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm | LayerNorm                  | 1 K   \n",
      "15 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention             | AlbertAttention            | 2 M   \n",
      "16 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query       | Linear                     | 590 K \n",
      "17 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key         | Linear                     | 590 K \n",
      "18 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value       | Linear                     | 590 K \n",
      "19 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dropout     | Dropout                    | 0     \n",
      "20 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense       | Linear                     | 590 K \n",
      "21 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm   | LayerNorm                  | 1 K   \n",
      "22 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn                   | Linear                     | 2 M   \n",
      "23 | model.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output            | Linear                     | 2 M   \n",
      "24 | model.albert.pooler                                                              | Linear                     | 590 K \n",
      "25 | model.albert.pooler_activation                                                   | Tanh                       | 0     \n",
      "26 | model.qa_outputs                                                                 | Linear                     | 1 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382cb3cb88b04dbea4451593c5f6223b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_model = TweetModel(tokenizer=tokenizer,train_df=train_df, val_df=val_df,\n",
    "                        lr=3e-5, batch_size=4)\n",
    "\n",
    "# get_total_params(tm)\n",
    "# finetune_model(tm,4)\n",
    "# print_trainable_params(tm)\n",
    "# get_trainable_params(tm)\n",
    "\n",
    "name = 'albert-check'\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "                save_dir='ts-logs',\n",
    "                name = name\n",
    "            )\n",
    "\n",
    "early_stopping = EarlyStopping('val_loss',patience=5)\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.mkdir('saved_models')\n",
    "    \n",
    "checkpoint_callback = ModelCheckpoint('saved_models/', save_top_k=2)\n",
    "# checkpoint_callback = ModelCheckpoint(filepath=f'saved_models/{name}')\n",
    "\n",
    "trainer = pl.Trainer(min_epochs=10,\n",
    "                     overfit_pct=0.01,\n",
    "                     early_stop_callback=early_stopping,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "# tm.cuda()\n",
    "trainer.fit(tweet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'merges_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-390c2307ab1f>\u001b[0m in \u001b[0;36mget_train_val_loaders\u001b[0;34m(df, train_idx, val_idx, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mTweetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-390c2307ab1f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, max_len)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mmerges_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roberta-base/merges.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             wordpieces_prefix=' ')\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'merges_file'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train, train.sentiment), start=1): \n",
    "    print(f'Fold: {fold}')\n",
    "\n",
    "    model = TweetModel()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n",
    "    criterion = loss_fn    \n",
    "    dataloaders_dict = get_train_val_loaders(train, train_idx, val_idx, batch_size)\n",
    "\n",
    "    train_model(\n",
    "        model, \n",
    "        dataloaders_dict,\n",
    "        criterion, \n",
    "        optimizer, \n",
    "        num_epochs,\n",
    "        f'roberta_char_level_fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test['text'] = test['text'].astype(str)\n",
    "test_loader = get_test_loader(test)\n",
    "predictions = []\n",
    "models = []\n",
    "for fold in range(skf.n_splits):\n",
    "    model = TweetModel()\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(f'roberta_fold{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "for data in test_loader:\n",
    "    ids = data['ids'].cuda()\n",
    "    masks = data['masks'].cuda()\n",
    "    tweet = data['tweet']\n",
    "    offsets = data['offsets'].numpy()\n",
    "\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(ids, masks)\n",
    "            start_logits.append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n",
    "            end_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    start_logits = np.mean(start_logits, axis=0)\n",
    "    end_logits = np.mean(end_logits, axis=0)\n",
    "    for i in range(len(ids)):    \n",
    "        start_pred = np.argmax(start_logits[i])\n",
    "        end_pred = np.argmax(end_logits[i])\n",
    "        if start_pred > end_pred:\n",
    "            pred = tweet[i]\n",
    "        else:\n",
    "            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
    "sub_df['selected_text'] = predictions\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
    "sub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
